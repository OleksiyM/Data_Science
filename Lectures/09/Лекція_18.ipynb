{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Jc4GeqwXTPUq",
        "lgU_xTIES4Lq",
        "JM5RsHt7CCgw",
        "tLglQuJ29bjU",
        "CARmufqWAtxH",
        "jISr-SWpGJXn",
        "L9YsmIJz-nbH",
        "KZN5vYbSK8Jf",
        "qr0egdGBPCbS",
        "ytQgCdsw-5BQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tак Keras чи TensorFlow?\n"
      ],
      "metadata": {
        "id": "zGAYp8teSyLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keras** - це бібліотека верхнього рівня, з її допомогою можна конструювати нейромережі, маніпулюючи їх шарами та параметрами.\n",
        "\n",
        "Для роботи з тензорами та виконання над ними різних операцій вона покладається на бекенди. Останні спеціально оптимізовані під обчислення з використанням тензорів та інших багатовимірних об'єктів. В якості таких бекендів keras може використовувати такі бібліотеки Tensorflow, Theano, CNTK, etc.\n",
        "\n",
        "Код, написаний з використанням Keras, може бути запущений за допомогою будь-якого з перерахованих бекендів. Більше того, в процесі розробки між ними можна перемикатися, що може бути досить корисним, так як у певних ситуаціях одні бекенди працюють швидше, ніж інші. Ми ж далі весь час будемо використовувати tensorflow як найбільш поширений та популярний фреймворк.\n",
        "\n",
        "Якщо ми виконуємо код на **CPU tensorflow** сам по собі працює як обгортка над більш низькорівневою бібліотекою для тензорних операцій, яка називається **Eigen**.\n",
        "\n",
        "Якщо ж ми використовуємо для обчислень **GPU**, то тоді tensorflow використовує **NVIDIA CUDA Deep Neural Network library (cuDNN)** - це добре оптимізована низькорівнева бібліотека для глибокого навчання."
      ],
      "metadata": {
        "id": "pIvJO0W-Ri1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://textbook.edu.goit.global/python/data-science-remaster/v1/img/module-10/backends.png)"
      ],
      "metadata": {
        "id": "26e5SU8fRh-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"
      ],
      "metadata": {
        "id": "vJyREdeUUm4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Навчання моделі"
      ],
      "metadata": {
        "id": "Jc4GeqwXTPUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Структура.\n",
        "\n",
        "1. Імпорт.\n",
        "2. Завантаження даних.\n",
        "3. Підготовка даних.\n",
        "4. Аналіз даних.\n",
        "5. Створення архітектури моделі (Sequantial).\n",
        "6. Компілювання (.compile()).\n",
        "7. Навчання (.fit()).\n",
        "8. Оцінка.\n",
        "9. Збереження + деплой."
      ],
      "metadata": {
        "id": "bw77iiWuTpy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "# 2-3\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "#4\n",
        "\n",
        "# 5\n",
        "model = models.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 6\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 7\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# 8\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Точність на тестових даних:', test_acc)\n",
        "\n",
        "# 9\n",
        "model.save(\"my_model\")\n",
        "# model.save(\"my_model.h5\")\n",
        "\n",
        "# my_model_weights = model.get_weights()\n",
        "# model.save_weights(\"my_model_weights\")\n",
        "# model.save_weights(\"my_model_weights.h5\")\n",
        "\n",
        "# model.to_json(\"my_model.json\")\n",
        "\n",
        "# Завантаження\n",
        "# model = keras.models.load_model(\"my_model\")\n",
        "# model = keras.models.load_weights(\"my_model\")\n",
        "# model = keras.models.model_from_json(\"my_model.json\")"
      ],
      "metadata": {
        "id": "SkC8JkRzTKg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6430aa40-b73c-4609-8107-54faf0c4516e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 14s 6ms/step - loss: 0.2892 - accuracy: 0.9153\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1426 - accuracy: 0.9575\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1080 - accuracy: 0.9671\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0876 - accuracy: 0.9722\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0746 - accuracy: 0.9766\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0751 - accuracy: 0.9772\n",
            "Точність на тестових даних: 0.9771999716758728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = models.load_model(\"my_model\")\n",
        "new_model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7fxdMzuBlWZ",
        "outputId": "5427c905-79d7-4495-bffd-1bbb3d7a102c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0751 - accuracy: 0.9772\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07514239847660065, 0.9771999716758728]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit"
      ],
      "metadata": {
        "id": "FzBnS01VPNF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='rmsprop', # Стрінг (назва оптимізатора) або екземпляр оптимізатора\n",
        "    loss=None, # Функція втрат\n",
        "    metrics=None, # Список показників, які оцінює модель під час навчання та тестування.\n",
        "    loss_weights=None, # Додатковий список або словник із зазначенням скалярних коефіцієнтів (числа з плаваючою точкою Python) для зважування внесків втрат різних вихідних даних моделі.\n",
        "    weighted_metrics=None, # Список показників, які потрібно оцінити та зважити за sample_weight або class_weight під час навчання та тестування.\n",
        "    run_eagerly=None, # Якщо True, логіка цієї моделі не буде загорнута в tf.function.\n",
        "    steps_per_execution=None, # Кількість пакетів для запуску під час кожного виклику tf.function.\n",
        "    jit_compile=None, # Якщо True, скомпілюйте етап навчання моделі за допомогою XLA. XLA — це оптимізуючий компілятор для машинного навчання.\n",
        "    pss_evaluation_shards=0, # його аргумент встановлює кількість сегментів, на які потрібно розділити набір даних, щоб увімкнути точну гарантію відвідування для оцінки, тобто модель буде застосовано до кожного елемента набору даних точно один раз, навіть якщо працівники не впораються.\n",
        ")"
      ],
      "metadata": {
        "id": "Lk6xA6gIVIQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x=None, # Вхідні дані\n",
        "    y=None, # Цільові дані\n",
        "    batch_size=None, #  Кількість зразків на оновлення градієнта\n",
        "    epochs=1, # . Кількість епох для навчання моделі.\n",
        "    verbose='auto', # режим докладності.\n",
        "    callbacks=None, # Список зворотних викликів для застосування під час навчання.\n",
        "    validation_split=0.0, # Частка даних навчання, які будуть використовуватися як дані перевірки.\n",
        "    validation_data=None,  # Дані для оцінки втрат і будь-яких показників моделі в кінці кожної епохи\n",
        "    shuffle=True, # Цей аргумент ігнорується, якщо x є генератором або об’єктом tf.data.Dataset.\n",
        "    class_weight=None, # Додатковий словник, що відображає індекси класу (цілі числа) на значення ваги (float), що використовується для зважування функції втрат (лише під час навчання)\n",
        "    sample_weight=None, # Додатковий масив ваг Numpy для навчальних зразків, що використовується для зважування функції втрат (лише під час навчання).\n",
        "    initial_epoch=0, # Епоха, з якої розпочинається тренування (корисно для відновлення попереднього тренування).\n",
        "    steps_per_epoch=None, # Загальна кількість кроків (пакетів зразків) до оголошення однієї епохи завершеною та початку наступної епохи.\n",
        "    validation_steps=None, # Загальна кількість кроків (пакетів зразків) для малювання перед зупинкою під час виконання перевірки в кінці кожної епохи.\n",
        "    validation_batch_size=None, # Кількість зразків на пакет перевірки\n",
        "    validation_freq=1,  # Якщо ціле число, вказує, скільки епох навчання потрібно виконати перед тим, як буде виконано новий запуск перевірки\n",
        "    max_queue_size=10, #Максимальний розмір черги генератора.\n",
        "    workers=1, # Максимальна кількість процесів для розкручування при використанні потоків на основі процесів.\n",
        "    use_multiprocessing=False # Якщо True, використовувати потоки на основі процесу.\n",
        ")"
      ],
      "metadata": {
        "id": "6sApY7ljTO9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Зміна оптимізатора"
      ],
      "metadata": {
        "id": "lgU_xTIES4Lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Нащо змінювати оптимізатор?"
      ],
      "metadata": {
        "id": "JM5RsHt7CCgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Градієнтний вибух (Gradient Explosion)** і **градієнтне затухання (Gradient Vanishing)** - це два проблеми, які можуть виникати при навчанні нейронних мереж, особливо у глибоких архітектурах."
      ],
      "metadata": {
        "id": "U-lKZVxoCSBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://miro.medium.com/v2/resize:fit:1400/1*EGWeyCcqVokP-XllKyPvVg.gif)"
      ],
      "metadata": {
        "id": "ig01IIrUEdfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Градієнтний вибух (Gradient Explosion):**\n",
        "\n",
        "- Це ситуація, коли значення градієнтів стають дуже великими під час навчання. Це може відбутися, наприклад, під час зворотного поширення помилки, коли градієнти передаються назад через декілька шарів і помножуються на кожному кроці.\n",
        "- Градієнтний вибух може призвести до розбіжності навчання, оскільки великі значення градієнтів можуть викликати перевантаження (overflow) числових типів даних і втрату стабільності процесу навчання."
      ],
      "metadata": {
        "id": "ZCjiMSyGCghS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Градієнтне затухання (Gradient Vanishing):**\n",
        "\n",
        "- Це протилежна проблема, коли значення градієнтів стають дуже малими під час навчання, особливо у глибоких архітектурах. Це може виникнути через декілька факторів, таких як велика кількість шарів, використання активаційних функцій з малим градієнтом (наприклад, сигмоїдальна або тангенс гіперболічний), або низька швидкість навчання.\n",
        "- Градієнтне затухання може призвести до того, що ваги моделі не оновлюються або оновлюються дуже повільно, що призводить до повільного навчання або навіть до того, що модель не навчається взагалі."
      ],
      "metadata": {
        "id": "uZBXMHN_CjWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.comet.com/site/blog/vanishing-exploding-gradients-in-deep-neural-networks/\n",
        "\n",
        "https://medium.com/dscier/how-to-deal-with-vanishing-and-exploding-gradients-in-neural-networks-24eb00c80e84"
      ],
      "metadata": {
        "id": "-xPcmZs5ETFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD"
      ],
      "metadata": {
        "id": "tLglQuJ29bjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оптимізатор **SGD (Stochastic Gradient Descent)** є одним з найпоширеніших методів оптимізації у машинному навчанні. Формула для оновлення ваг моделі за допомогою оптимізатора SGD можна записати наступним чином:\n",
        "\n",
        "$$w_{нове} = w_{поточне} - LearningRate * ∇J(w_{поточне}) $$\n",
        "\n",
        "$w_{нове}$ - нове значення параметра ваги,\n",
        "\n",
        "$w_{поточне}$ - поточне значення параметра ваги,\n",
        "\n",
        "$LearningRate$ - швидкість навчання (learning rate), яка визначає, наскільки швидко модель навчається\n",
        "\n",
        "$∇J$ - градієнт функції втрат $J$ відносно параметра ваги $w_{поточне}$\n",
        "\n",
        "$∇J$ обчислюється шляхом знаходження похідних функції втрат за кожним параметром ваги відносно вхідних даних."
      ],
      "metadata": {
        "id": "1TCkQlwW_QYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/SGD\n"
      ],
      "metadata": {
        "id": "chNP8AY0BUnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) # Компіляція моделі з оптимізатором SGD\n",
        "# optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Точність на тестових даних:', test_acc)"
      ],
      "metadata": {
        "id": "EeChfrksAU9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f17d67-d738-4e64-9e30-18194d269c9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6981 - accuracy: 0.8089\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3757 - accuracy: 0.8929\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3171 - accuracy: 0.9087\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2798 - accuracy: 0.9201\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2543 - accuracy: 0.9273\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2048 - accuracy: 0.9413\n",
            "Точність на тестових даних: 0.9412999749183655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Переваги SGD:**\n",
        "\n",
        "- Простота: Алгоритм SGD є відносно простим у реалізації та розумінні. Це один з основних алгоритмів оптимізації, який зазвичай вивчається в курсах з машинного навчання.\n",
        "- Ефективність на великих наборах даних: SGD може бути ефективним на великих наборах даних, оскільки він оновлює параметри моделі по частинам даних, що дозволяє швидше навчати модель.\n",
        "- Можливість уникнути локальних мінімумів: Завдяки стохастичності та випадковому вибору партій даних, SGD може допомогти уникнути застрягання в локальних мінімумах функції втрат.\n",
        "\n",
        "**Недоліки SGD:**\n",
        "\n",
        "- Нестійкість: SGD може бути нестійким, оскільки він може змінювати напрямок оновлення на кожному кроці. Це може призвести до мінливого навчання та ускладнити збіжність алгоритму.\n",
        "- Швидкість збіжності: У порівнянні з більш складними методами оптимізації, SGD може мати меншу швидкість збіжності, особливо на багатовимірних та складних просторах параметрів.\n",
        "- Чутливість до гіперпараметрів: Швидкість навчання (learning rate) впливає на швидкість збіжності та стабільність алгоритму SGD. Встановлення відповідного значення learning rate може бути нетривіальною задачею, і невірне значення може призвести до поганої збіжності або розбіжності."
      ],
      "metadata": {
        "id": "ENw6HS0HA9Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGDMomentum"
      ],
      "metadata": {
        "id": "CARmufqWAtxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SGD з моментом (SGD with Momentum)** - це вдосконалена версія стохастичного градієнтного спуску (SGD), яка дозволяє розглядати не тільки поточний градієнт, але й попередні оновлення ваг для керування швидкістю та напрямком оптимізації.\n",
        "\n",
        "$$w_{нове} = w_{поточне} - LearningRate * ∇J(w_{поточне}) * momentum $$\n",
        "$$ momentum =β⋅v_t+(1−β)$$\n",
        "\n",
        "$v_t$ -  швидкість оновлення (момент).\n",
        "\n",
        "$β$ -  коефіцієнт моменту, який зазвичай має значення від 0 до 1."
      ],
      "metadata": {
        "id": "B3QSpBYaB1g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9) # Компіляція моделі з оптимізатором SGDMomentum\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Точність на тестових даних:', test_acc)"
      ],
      "metadata": {
        "id": "Ju_REWQtFdrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589a1214-2d2c-4e2b-f2a5-a43b506b58e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3368 - accuracy: 0.9009\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1720 - accuracy: 0.9498\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1330 - accuracy: 0.9617\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1111 - accuracy: 0.9672\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0964 - accuracy: 0.9711\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0812 - accuracy: 0.9746\n",
            "Точність на тестових даних: 0.9746000170707703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Переваги:**\n",
        "\n",
        "- Прискорення збіжності: Момент допомагає прискорити процес збіжності, особливо в областях з поганим градієнтом або в узких мінімумах функції втрат.\n",
        "\n",
        "- Стабільність швидкості навчання: Момент допомагає зберегти стабільну швидкість навчання в напрямку найшвидшого спуску.\n",
        "\n",
        "- Зменшення збурень: Момент зменшує вплив великих або випадкових градієнтів, дозволяючи більш стабільний та менш шумний процес оптимізації.\n",
        "\n",
        "**Недоліки:**\n",
        "\n",
        "- Необхідність налаштування параметрів: Параметр моменту (momentum) потрібно налаштовувати, і невірно обране значення може призвести до повільного навчання або нестабільності.\n",
        "\n",
        "- Можливість втрати величини градієнта: При неправильному налаштуванні параметра моменту може виникнути ситуація, коли швидкість навчання стає занадто великою або занадто малою, що може призвести до втрати важливих градієнтів та повільного навчання."
      ],
      "metadata": {
        "id": "Im170-nsB1nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGDNesterov (SGD with Nesterov Momentum/SGD with Nesterov Accelerated Gradient)"
      ],
      "metadata": {
        "id": "jISr-SWpGJXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SGD з Nesterov Accelerated Gradient (SGD with Nesterov Momentum)** є варіацією алгоритму стохастичного градієнтного спуску (SGD) з моментом, яка використовує оновлення градієнту, враховуючи не тільки поточе значення градієнту, але і \"передбачене\" значення градієнту в майбутньому. Це допомагає алгоритму здійснювати оновлення в напрямку, що можливо більш точно вказує на глобальний мінімум функції втрат.\n",
        "\n",
        "\n",
        "$$w_{нове} = w_{поточне} - β⋅v_t + LearningRate * ∇J(w_{поточне}- β⋅v_t) $$\n",
        "\n",
        "$v_t$ -  швидкість оновлення (момент).\n",
        "\n",
        "$β$ -  коефіцієнт моменту, який зазвичай має значення від 0 до 1.\n",
        "\n",
        "$w_{поточне}- β⋅v_t$ - \"передбачене\" значення градієнту в майбутньому"
      ],
      "metadata": {
        "id": "J0EuU-uzGWD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Тренування моделі\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Оцінка моделі на тестових даних\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Точність на тестових даних:', test_acc)"
      ],
      "metadata": {
        "id": "0frZFR5XHuZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f60b61-9415-422b-f83a-0d77ab068d4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3311 - accuracy: 0.9043\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1692 - accuracy: 0.9508\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1323 - accuracy: 0.9609\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1094 - accuracy: 0.9675\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0961 - accuracy: 0.9712\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0825 - accuracy: 0.9747\n",
            "Точність на тестових даних: 0.9746999740600586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Переваги:**\n",
        "\n",
        "- Швидше збіжність: Nesterov Momentum може прискорити процес навчання, оскільки враховується \"передбачене\" значення градієнту в майбутньому. Це дозволяє алгоритму краще орієнтуватися в напрямку оптимізації і швидше знаходити локальні мінімуми.\n",
        "\n",
        "- Більш стабільне навчання: Використання Nesterov Momentum допомагає зменшити вплив великих градієнтів, що дозволяє більш стабільно навчати модель, особливо на великих даних або в глибоких нейронних мережах.\n",
        "\n",
        "- Менше затримок у навчанні: За рахунок використання \"передбаченого\" значення градієнту в майбутньому, Nesterov Momentum може допомогти уникнути затримок у навчанні, коли градієнти мають велику дисперсію.\n",
        "\n",
        "**Недоліки:**\n",
        "\n",
        "- Необхідність налаштування параметрів: Як і в інших алгоритмах оптимізації, для досягнення оптимальних результатів необхідно належним чином налаштувати параметри, такі як коефіцієнт моменту і швидкість навчання.\n",
        "\n",
        "- Підвищена обчислювальна складність: Nesterov Momentum вимагає обчислення \"передбаченого\" значення градієнту в майбутньому, що може призвести до додаткових обчислювальних витрат порівняно зі звичайним методом SGD з моментом.\n",
        "\n",
        "- Підвищена чутливість до шуму: В деяких випадках Nesterov Momentum може бути більш чутливим до шуму в даних, оскільки враховує \"передбачені\" значення градієнту. Це може призвести до менш стійкої поведінки алгоритму на деяких наборах даних."
      ],
      "metadata": {
        "id": "qyKPBLEsGWGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://miro.medium.com/v2/resize:fit:1027/1*6MEi74EMyPERHlAX-x2Slw.png)"
      ],
      "metadata": {
        "id": "i0nob9h9GWIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12"
      ],
      "metadata": {
        "id": "E6BHbx5KGWLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adagrad"
      ],
      "metadata": {
        "id": "L9YsmIJz-nbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adagrad (Adaptive Gradient Algorithm)** - це адаптивний алгоритм оптимізації, який адаптує швидкість навчання (learning rate) для кожного параметра ваги на основі історії градієнтів, що оновлюється. **Чим більше флуктуацій в даних тим меньше стає шаг.**.\n",
        "\n",
        "$$ \\omega_{t+1,i} = \\omega_{t,i} - \\frac{\\text{learning_rate}}{\\sqrt{G_{t,ii} + \\epsilon}} \\cdot g_{t,i}$$\n",
        "\n",
        "$\\omega_{t,i}$- поточне значення параметра ваги i на кроці t,\n",
        "\n",
        "$\\text{learning_rate}$ - швидкість навчання (learning rate),\n",
        "\n",
        "$g_{t,i}$ - градієнт функції втрат відносно параметра ваги i на кроці t,\n",
        "\n",
        "$G_{t,ii}$ - накопичений квадрат градієнтів для параметра ваги i до кроку t,\n",
        "\n",
        "$\\epsilon$ - додатковий параметр для стабілізації."
      ],
      "metadata": {
        "id": "DWSHGBEAGJB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "#optimizer = tf.keras.optimizers.experimental.Adagrad(learning_rate=0.01)\n",
        "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Тренування моделі\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Оцінка моделі на тестових даних\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Точність на тестових даних:', test_acc)"
      ],
      "metadata": {
        "id": "zTFHJM4xKfH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c940a8-0595-4ad0-e2e4-4e0429fb9275"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5008 - accuracy: 0.8606\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3005 - accuracy: 0.9154\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2532 - accuracy: 0.9288\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2216 - accuracy: 0.9377\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2039 - accuracy: 0.9431\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1660 - accuracy: 0.9521\n",
            "Точність на тестових даних: 0.9520999789237976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/Adagrad"
      ],
      "metadata": {
        "id": "n6bTRgspI51q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Переваги:**\n",
        "\n",
        "- Адаптивність швидкості навчання: Adagrad використовує історію градієнтів для адаптації швидкості навчання для кожного параметра ваги. Це дозволяє більш точно налаштовувати швидкість навчання для кожного параметра залежно від його історії оновлень.\n",
        "\n",
        "- Ефективне навчання для рідкісних функцій втрат: Adagrad добре працює з рідкісними функціями втрат, оскільки великі градієнти автоматично зменшують свої швидкості навчання, тоді як малим градієнтам надається більше уваги.\n",
        "\n",
        "- Відсутність необхідності вручну налаштовувати швидкість навчання: Оскільки швидкість навчання адаптується автоматично для кожного параметра, немає потреби вручну налаштовувати її.\n",
        "\n",
        "**Недоліки:**\n",
        "\n",
        "- Накопичення квадратів градієнтів: Adagrad накопичує квадрати градієнтів в знаменнику під час навчання. Це може призвести до зростання значень цього знаменника з часом, що призводить до уповільнення швидкості навчання з часом і може викликати проблему великих значень швидкості навчання, особливо для рідкісних функцій втрат.\n",
        "\n",
        "- Потенційний розрив у навчанні: Оскільки знаменник Adagrad накопичує квадрати градієнтів, великі значення квадратів градієнтів можуть призводити до дуже малих або навіть нульових значень швидкості навчання, що може призвести до зупинки навчання.\n",
        "\n",
        "- Неадекватна швидкість навчання для поганих градієнтів: Швидкість навчання для параметрів з великими градієнтами може стати надто мала, що призведе до повільного навчання для цих параметрів."
      ],
      "metadata": {
        "id": "-abZ4wafI536"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adadelta"
      ],
      "metadata": {
        "id": "KZN5vYbSK8Jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adadelta** - це адаптивний алгоритм оптимізації, який є вдосконаленням Adagrad. Він вирішує деякі недоліки Adagrad, зокрема проблему накопичення квадратів градієнтів у знаменнику, що може призводити до зменшення швидкості навчання з часом. Основні ідеї Adadelta полягають у тому, щоб обмежити обсяг історії градієнтів, зберігаючи тільки обмежену кількість попередніх оновлень.\n",
        "\n",
        "$$ \\omega_{t+1,i} = \\omega_{t,i} - \\frac{\\text{learning_rate}}{\\sqrt{G_{t,ii} + \\epsilon}} \\cdot g_{t,i}$$\n",
        "\n",
        "$$G_{t,ii} = ρ*G_{t-1,ii} + (1-ρ)g_{t,i}^2 $$\n",
        "\n",
        "$ρ$ - швидкість ослаблення [0:1]"
      ],
      "metadata": {
        "id": "HmGH-DRPLLj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/Adadelta"
      ],
      "metadata": {
        "id": "lNROsNu0NIRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "#optimizer = tf.keras.optimizers.experimental.Adadelta(learning_rate=1.0)\n",
        "optimizer = tf.keras.optimizers.Adadelta(learning_rate=1.0, rho = 0.95)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Тренування моделі\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Оцінка моделі на тестових даних\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Точність на тестових даних:', test_acc)"
      ],
      "metadata": {
        "id": "zFbZFNgDM241",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf10fea-01dd-4217-8eed-ac8e47cc2819"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3022 - accuracy: 0.9132\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1603 - accuracy: 0.9536\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1248 - accuracy: 0.9635\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1049 - accuracy: 0.9689\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0935 - accuracy: 0.9724\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.9756\n",
            "Точність на тестових даних: 0.975600004196167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Переваги Adadelta:**\n",
        "\n",
        "- Безпека швидкості навчання: Adadelta автоматично адаптує швидкість навчання для кожного параметра, що дозволяє покращити стабільність та збіжність алгоритму.\n",
        "\n",
        "- Відсутність необхідності вручну налаштовувати гіперпараметри: Відсутність необхідності налаштовувати гіперпараметр швидкості навчання робить Adadelta менш чутливим до вибору гіперпараметрів.\n",
        "\n",
        "- Ефективне навчання для рідкісних функцій втрат: Алгоритм Adadelta ефективний для рідкісних функцій втрат через адаптивну швидкість навчання.\n",
        "\n",
        "**Недоліки Adadelta:**\n",
        "\n",
        "- Потенційний розрив у навчанні: Як і Adagrad, Adadelta може стикається з проблемою зменшення швидкості навчання з часом, що може впливати на швидкість збіжності.\n",
        "\n",
        "- Підвищена обчислювальна складність: Обчислення квадратів градієнтів та попередніх оновлень може призвести до додаткових обчислювальних витрат."
      ],
      "metadata": {
        "id": "nUF-Dk-0LLvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSProp"
      ],
      "metadata": {
        "id": "qr0egdGBPCbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMSProp (Root Mean Square Propagation)** - це адаптивний алгоритм оптимізації, який використовує інформацію про середньоквадратичне значення попередніх градієнтів для адаптації швидкості навчання. Він був розроблений для розв'язання проблеми зниження швидкості навчання в алгоритмі Adagrad, що виникає при великих кількостях оновлень.\n",
        "\n",
        "$$\\omega_{t+1,i} = \\omega_{t,i} - \\frac{\\text{learning_rate}}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot g_{t,i}$$\n",
        "\n",
        "$$E[g^2]_t = \\rho \\cdot E[g^2]_{t-1} + (1 - \\rho) \\cdot g^2_t $$\n",
        "\n",
        "$E[g^2]_{t}$ - середнє квадратичне значення градієнтів на кроці t,"
      ],
      "metadata": {
        "id": "JlPC7A_oPE40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/RMSprop"
      ],
      "metadata": {
        "id": "FGLNvCH8PE7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "#optimizer = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.01, rho=0.95)\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01, rho=0.95)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Тренування моделі\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Оцінка моделі на тестових даних\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Точність на тестових даних:', test_acc)"
      ],
      "metadata": {
        "id": "hw75T-5fPbLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "658239f2-2f6d-4f84-a83b-dff69fe49abd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3574 - accuracy: 0.9028\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2671 - accuracy: 0.9364\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2500 - accuracy: 0.9443\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2417 - accuracy: 0.9492\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2202 - accuracy: 0.9538\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2073 - accuracy: 0.9620\n",
            "Точність на тестових даних: 0.9620000123977661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Переваги RMSProp:**\n",
        "\n",
        "- Адаптивна швидкість навчання: RMSProp використовує інформацію про середнє квадратичне значення попередніх градієнтів для адаптації швидкості навчання. Це дозволяє алгоритму більш ефективно навчати модель, особливо на великих наборах даних або в глибоких нейронних мережах.\n",
        "\n",
        "- Відсутність проблеми зниження швидкості навчання: Використання експоненційного згладжування дозволяє RMSProp уникнути проблеми зниження швидкості навчання в алгоритмі Adagrad.\n",
        "\n",
        "- Відносно простий алгоритм: RMSProp є відносно простим алгоритмом оптимізації, що легко імплементувати та налаштовувати.\n",
        "\n",
        "**Недоліки RMSProp:**\n",
        "\n",
        "- Залежність від початкових значень градієнтів: RMSProp може бути чутливим до початкових значень градієнтів, особливо якщо вони дуже великі або дуже малі.\n",
        "\n",
        "- Підвищена обчислювальна складність: Обчислення квадратів градієнтів та їх середніх може призвести до підвищеної обчислювальної складності, особливо на великих наборах даних або в глибоких мережах."
      ],
      "metadata": {
        "id": "08KeJ9E3PE9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adam"
      ],
      "metadata": {
        "id": "ytQgCdsw-5BQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Adam (Adaptive Moment Estimation)** - це адаптивний алгоритм оптимізації, який комбінує ідеї алгоритмів RMSProp та Momentum. Він використовує експоненціально згладжені оцінки градієнту та квадрату градієнту для адаптивного налаштування швидкості навчання для кожного параметра.\n",
        "\n"
      ],
      "metadata": {
        "id": "x2qfpTP5-ndn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\omega_{t+1} = \\omega_t - \\frac{learning_rate * \\hat{m}_t} {(\\sqrt{\\hat{v}_t} + \\epsilon))}$$\n",
        "\n",
        "$$\\hat{m}_t = \\frac{\\beta_1*m_{t-1}+(1-\\beta_1)*g_t}{1-\\beta^t_1}$$\n",
        "$$\\hat{v}_t = \\frac{\\beta_2*v_{t-1}+(1-\\beta_2)*g^2_t}{1-\\beta^t_2}$$\n",
        "\n",
        "$\\hat{m}_t$ - виправлена оцінка першого моменту (середнє експоненціальне градієнтів),\n",
        "\n",
        "$\\hat{v}_t$ - виправлена оцінка другого моменту (середнє експоненціальне квадратів градієнтів),\n",
        "\n",
        "$m_t$ - експоненціально згладжена оцінка градієнту,\n",
        "\n",
        "$v_t$ - експоненціально згладжена оцінка квадрату градієнту,\n",
        "\n",
        "$\\beta_1, \\beta_2$ - коефіцієнти згладжування (зазвичай 0.9 та 0.999 відповідно),"
      ],
      "metadata": {
        "id": "ac24Bkv0SX05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"
      ],
      "metadata": {
        "id": "vk_mpMhpUeIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Тренування моделі\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Оцінка моделі на тестових даних\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Точність на тестових даних:', test_acc)"
      ],
      "metadata": {
        "id": "n0ueKG2zRTbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c7ddc2-3c44-4b50-88b6-af51115d3a08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.2940 - accuracy: 0.9146\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1441 - accuracy: 0.9567\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1074 - accuracy: 0.9676\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0863 - accuracy: 0.9737\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0753 - accuracy: 0.9769\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0765 - accuracy: 0.9774\n",
            "Точність на тестових даних: 0.977400004863739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Переваги Adam:**\n",
        "\n",
        "- Адаптивна швидкість навчання: Adam автоматично адаптує швидкість навчання для кожного параметра, що дозволяє ефективно навчати модель у великих масштабах або у глибоких нейронних мережах.\n",
        "- Ефективність: Adam комбінує переваги алгоритмів RMSProp та Momentum, що дозволяє швидше збіжність навчання та більш ефективну оптимізацію.\n",
        "- Відносно низька чутливість до вибору гіперпараметрів: Adam зазвичай працює добре з різними значеннями гіперпараметрів, що дозволяє позбутися необхідності вручну налаштовувати їх.\n",
        "\n",
        "**Недоліки Adam:**\n",
        "\n",
        "- Потреба у пам'яті: Adam вимагає збереження додаткових параметрів для обчислення експоненціально згладжених оцінок, що може призвести до більшого використання пам'яті, особливо при обробці великих даних або глибоких моделей."
      ],
      "metadata": {
        "id": "0THnOjbt9bl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сallbacks"
      ],
      "metadata": {
        "id": "e9EV3G3tS61o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Callback-функції** в TensorFlow - це інструмент, який дозволяє виконувати певні дії на різних етапах навчання моделі, таких як початок чи закінчення епохи, кожна ітерація навчання тощо. Вони використовуються для моніторингу, збереження моделі, а також для визначення зупинки навчання в залежності від деяких умов."
      ],
      "metadata": {
        "id": "o20OMvC_XKrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback"
      ],
      "metadata": {
        "id": "3BbB3L-UiAg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ModelCheckpoint: Збереження моделі під час навчання. Цей колбек дозволяє зберегти модель у файлі після кожної епохи або при кращих результатів на валідаційному наборі даних.\n",
        "\n"
      ],
      "metadata": {
        "id": "XXJb9vxvhvi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
        "# Створення callback-функції ModelCheckpoint\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5',\n",
        "                                                          monitor='val_accuracy',\n",
        "                                                          save_best_only=True,\n",
        "                                                          verbose=1)\n",
        "\n",
        "# Тренування моделі з використанням ModelCheckpoint\n",
        "model.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels),\n",
        "          callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rEU-dQIidbX",
        "outputId": "2b26bb78-486a-41e7-b6e2-586827a96c3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.9165\n",
            "Epoch 1: val_accuracy improved from -inf to 0.95790, saving model to best_model.h5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2931 - accuracy: 0.9165 - val_loss: 0.1383 - val_accuracy: 0.9579\n",
            "Epoch 2/5\n",
            "  42/1875 [..............................] - ETA: 6s - loss: 0.1672 - accuracy: 0.9524"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9587\n",
            "Epoch 2: val_accuracy improved from 0.95790 to 0.96860, saving model to best_model.h5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1399 - accuracy: 0.9587 - val_loss: 0.1035 - val_accuracy: 0.9686\n",
            "Epoch 3/5\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.1054 - accuracy: 0.9678\n",
            "Epoch 3: val_accuracy improved from 0.96860 to 0.97550, saving model to best_model.h5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1054 - accuracy: 0.9678 - val_loss: 0.0802 - val_accuracy: 0.9755\n",
            "Epoch 4/5\n",
            "1860/1875 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9731\n",
            "Epoch 4: val_accuracy did not improve from 0.97550\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0868 - accuracy: 0.9731 - val_loss: 0.0796 - val_accuracy: 0.9739\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9769\n",
            "Epoch 5: val_accuracy improved from 0.97550 to 0.97890, saving model to best_model.h5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0735 - accuracy: 0.9769 - val_loss: 0.0689 - val_accuracy: 0.9789\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a521b5782b0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. EarlyStopping: Зупинка навчання, якщо покращення на валідаційному наборі даних не спостерігається протягом певної кількості епох.\n",
        "\n"
      ],
      "metadata": {
        "id": "121Ar7vJidnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
        "\n",
        "# Тренування моделі з використанням EarlyStopping\n",
        "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels),\n",
        "          callbacks=[early_stopping_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohCUf2_dieP8",
        "outputId": "211b3ca1-2402-4a40-e5cb-16fb17213f36"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2961 - accuracy: 0.9135 - val_loss: 0.1460 - val_accuracy: 0.9563\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1488 - accuracy: 0.9550 - val_loss: 0.1045 - val_accuracy: 0.9673\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1127 - accuracy: 0.9658 - val_loss: 0.0887 - val_accuracy: 0.9722\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0920 - accuracy: 0.9720 - val_loss: 0.0806 - val_accuracy: 0.9740\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0775 - accuracy: 0.9759 - val_loss: 0.0789 - val_accuracy: 0.9751\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0684 - accuracy: 0.9780 - val_loss: 0.0792 - val_accuracy: 0.9759\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a521b5667a0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. TensorBoard: Запуск TensorBoard, щоб візуалізувати метрики навчання та структуру моделі.\n",
        "\n"
      ],
      "metadata": {
        "id": "fmIRFVrBidqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
        "# Створення callback-функції TensorBoard\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1,write_graph=True, write_images=False)\n",
        "\n",
        "# Тренування моделі з використанням TensorBoard\n",
        "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels),\n",
        "          callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JP77eGeieis",
        "outputId": "8212cf43-df87-4dce-982d-0e84362ea6a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2996 - accuracy: 0.9126 - val_loss: 0.1447 - val_accuracy: 0.9566\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1462 - accuracy: 0.9564 - val_loss: 0.1008 - val_accuracy: 0.9691\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1108 - accuracy: 0.9661 - val_loss: 0.0863 - val_accuracy: 0.9743\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0905 - accuracy: 0.9718 - val_loss: 0.0792 - val_accuracy: 0.9753\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0769 - accuracy: 0.9763 - val_loss: 0.0775 - val_accuracy: 0.9761\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0666 - accuracy: 0.9796 - val_loss: 0.0753 - val_accuracy: 0.9779\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0601 - accuracy: 0.9806 - val_loss: 0.0738 - val_accuracy: 0.9780\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0534 - accuracy: 0.9826 - val_loss: 0.0786 - val_accuracy: 0.9774\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0487 - accuracy: 0.9842 - val_loss: 0.0659 - val_accuracy: 0.9803\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0452 - accuracy: 0.9851 - val_loss: 0.0740 - val_accuracy: 0.9790\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a5220e45030>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://i.stack.imgur.com/t7nhp.png)"
      ],
      "metadata": {
        "id": "Wwv0xx5wleUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. LearningRateScheduler: Зміна швидкості навчання в залежності від поточного епохи.\n",
        "\n"
      ],
      "metadata": {
        "id": "HfPu65u9idsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler\n",
        "# Функція для регулювання швидкості навчання\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 5:\n",
        "        return 0.01\n",
        "    elif epoch < 10:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.0001\n",
        "# Створення callback-функції LearningRateScheduler\n",
        "lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Тренування моделі з використанням LearningRateScheduler\n",
        "model.fit(train_images, train_labels, epochs=15, validation_data=(test_images, test_labels),\n",
        "          callbacks=[lr_scheduler_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leLiKn87ie7t",
        "outputId": "b57f99dc-b1fb-4211-9bf4-28f73c7f89a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3093 - accuracy: 0.9084 - val_loss: 0.1931 - val_accuracy: 0.9449 - lr: 0.0100\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2462 - accuracy: 0.9318 - val_loss: 0.2017 - val_accuracy: 0.9458 - lr: 0.0100\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2190 - accuracy: 0.9393 - val_loss: 0.1753 - val_accuracy: 0.9588 - lr: 0.0100\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2196 - accuracy: 0.9421 - val_loss: 0.1732 - val_accuracy: 0.9605 - lr: 0.0100\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2083 - accuracy: 0.9448 - val_loss: 0.1680 - val_accuracy: 0.9612 - lr: 0.0100\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1196 - accuracy: 0.9650 - val_loss: 0.1273 - val_accuracy: 0.9738 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0962 - accuracy: 0.9713 - val_loss: 0.1221 - val_accuracy: 0.9734 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0838 - accuracy: 0.9750 - val_loss: 0.1217 - val_accuracy: 0.9756 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0765 - accuracy: 0.9757 - val_loss: 0.1252 - val_accuracy: 0.9748 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0772 - accuracy: 0.9757 - val_loss: 0.1230 - val_accuracy: 0.9758 - lr: 0.0010\n",
            "Epoch 11/15\n",
            " 427/1875 [=====>........................] - ETA: 6s - loss: 0.0693 - accuracy: 0.9775"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. CSVLogger: Запис метрик навчання у CSV-файл.\n",
        "\n"
      ],
      "metadata": {
        "id": "30zhzYu5idu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CSVLogger\n",
        "# Створення callback-функції CSVLogger\n",
        "csv_logger_callback = tf.keras.callbacks.CSVLogger(filename='training_log.csv')\n",
        "\n",
        "# Тренування моделі з використанням CSVLogger\n",
        "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels),\n",
        "          callbacks=[csv_logger_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1nfHHTjifPs",
        "outputId": "8513f257-6663-40cc-8006-70354dde5bcc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2935 - accuracy: 0.9146 - val_loss: 0.1375 - val_accuracy: 0.9587\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1405 - accuracy: 0.9597 - val_loss: 0.1022 - val_accuracy: 0.9672\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1054 - accuracy: 0.9677 - val_loss: 0.0883 - val_accuracy: 0.9713\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0859 - accuracy: 0.9740 - val_loss: 0.0788 - val_accuracy: 0.9757\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0737 - accuracy: 0.9768 - val_loss: 0.0724 - val_accuracy: 0.9774\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0626 - accuracy: 0.9802 - val_loss: 0.0745 - val_accuracy: 0.9778\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0567 - accuracy: 0.9815 - val_loss: 0.0712 - val_accuracy: 0.9767\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0503 - accuracy: 0.9837 - val_loss: 0.0692 - val_accuracy: 0.9798\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0473 - accuracy: 0.9844 - val_loss: 0.0680 - val_accuracy: 0.9794\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0434 - accuracy: 0.9856 - val_loss: 0.0677 - val_accuracy: 0.9810\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a522108fdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "6. ReduceLROnPlateau: Зменшення швидкості навчання, якщо покращення на валідаційному наборі даних не спостерігається протягом певної кількості епох."
      ],
      "metadata": {
        "id": "ElpRpb55inq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження даних\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Створення та компіляція моделі\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau\n",
        "# Створення callback-функції ReduceLROnPlateau\n",
        "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "'''\n",
        "У цьому прикладі, якщо протягом 2 епох покращення на валідаційному наборі не буде спостережено,\n",
        "швидкість навчання буде зменшена у 5 разів (factor=0.2).\n",
        "Це допомагає у збільшенні стабільності та збіжності під час навчання моделі.\n",
        "'''\n",
        "# Тренування моделі з використанням ReduceLROnPlateau\n",
        "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels),\n",
        "          callbacks=[reduce_lr_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWoSWcsOinvr",
        "outputId": "3d02aa0c-80ec-48f7-fe02-1d6a41a487a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2921 - accuracy: 0.9159 - val_loss: 0.1345 - val_accuracy: 0.9599 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1398 - accuracy: 0.9580 - val_loss: 0.0933 - val_accuracy: 0.9703 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1079 - accuracy: 0.9668 - val_loss: 0.0870 - val_accuracy: 0.9723 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0872 - accuracy: 0.9733 - val_loss: 0.0822 - val_accuracy: 0.9734 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0730 - accuracy: 0.9766 - val_loss: 0.0776 - val_accuracy: 0.9768 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0634 - accuracy: 0.9796 - val_loss: 0.0754 - val_accuracy: 0.9766 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0580 - accuracy: 0.9820 - val_loss: 0.0692 - val_accuracy: 0.9779 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0518 - accuracy: 0.9833 - val_loss: 0.0783 - val_accuracy: 0.9771 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0475 - accuracy: 0.9843 - val_loss: 0.0671 - val_accuracy: 0.9789 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0434 - accuracy: 0.9858 - val_loss: 0.0741 - val_accuracy: 0.9795 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a5221101ae0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Практичний нотбук"
      ],
      "metadata": {
        "id": "FCr5krl5uYFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/yassineghouzam/introduction-to-cnn-keras-0-997-top-6/notebook"
      ],
      "metadata": {
        "id": "0SksVh8ynXaq"
      }
    }
  ]
}